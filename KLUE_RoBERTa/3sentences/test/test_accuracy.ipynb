from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
print('Available device : ', torch.cuda.device_count())
print('Current cuda device :', torch.cuda.current_device())
print(torch.cuda.get_device_name(device))

# Setting parameters
batch_size = 300
num_label = 7
learning_rate = 1e-5
max_grad_norm = 1
epochs = 2
num_workers = 8
# max_len = 512

import time
start_time = time.time()

from transformers import BertTokenizer, RobertaTokenizer, RobertaForSequenceClassification
import torch

config = 'klue/roberta-large'
tokenizer = BertTokenizer.from_pretrained(config)
model = RobertaForSequenceClassification.from_pretrained(config, num_labels=num_label)

model.to(device)

model_save_dir='/home/yeji/standard/model/topic3/3sentences/saved_model/20211127_195801/epoch_6.pt'

learning_rate = 1e-5
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)


checkpoint = torch.load(model_save_dir)
# 원래 module. 없어야됨
# 근데 생겨서 지움
new_checkpoint = { k.replace('module.','') if 'module.' in k else k:v for k,v in checkpoint['model_state_dict'].items()}

model.load_state_dict(new_checkpoint)#checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']

model.eval()
print('Model loaded!')

import pandas as pd

test_path = '/home/yeji/standard/model/topic3/3sentences/experiment_dataset/topic_train_dataset.csv'
test_df = pd.read_csv(test_path)  

from torch.utils.data import Dataset, DataLoader

class Dataset(Dataset):
    def __init__(self, df):
        self.df = df

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        text = self.df.iloc[idx, 0]
        #         print(text)

        label = self.df.iloc[idx, 1]
        #         print(label)
        return text, label

test_dataset = Dataset(test_df)  
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)

import torch.nn.functional as F
from tqdm import tqdm

def test():
    model.eval()

    total_loss = 0
    total_len = 0
    total_correct = 0

    for text, label in tqdm(test_loader):
        # print("Running Validation...")

        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]
        padded_list = [e + [0] * (512 - len(e)) for e in encoded_list]

        sample = torch.tensor(padded_list)
        sample, label = sample.to(device), label.to(device)
        labels = label.clone().detach()

        with torch.no_grad():
            outputs = model(sample, labels=labels)
        _, logits = outputs

        pred = torch.argmax(F.softmax(logits, dim=1), dim=1)
        correct = pred.eq(labels)
        total_correct += correct.sum().item()
        total_len += len(labels)

    print('Test accuracy: {:.4f}'.format(total_correct / total_len))
    
    now = time.gmtime(time.time() - start_time)
    print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))
    
test()

# 100
