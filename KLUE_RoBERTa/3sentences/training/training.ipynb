from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
# from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AdamW
from transformers import BertTokenizer, RobertaForSequenceClassification

# from torch.optim import Adam
import torch.nn.functional as F
import time
import os
import datetime
from datetime import datetime
from transformers import get_linear_schedule_with_warmup
import random  # 재현을 위해 랜덤시드 고정
import tqdm

os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # Uses GPU 0.
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
print('Available device : ', torch.cuda.device_count())
print('Current cuda device :', torch.cuda.current_device())
print(torch.cuda.get_device_name(device))



# Setting parameters
batch_size = 16
num_label = 7
learning_rate = 1e-5
max_grad_norm = 1
epochs = 10
num_workers = 8
# max_len = 512



start_time = time.time()

from transformers import BertTokenizer, RobertaTokenizer, RobertaForSequenceClassification
import torch

config = 'klue/roberta-large'
tokenizer = BertTokenizer.from_pretrained(config)
model = RobertaForSequenceClassification.from_pretrained(config, num_labels=num_label)




import pandas as pd

train_path ='./experiment_dataset/topic_train_dataset.csv'
valid_path = './experiment_dataset/topic_valid_dataset.csv'

train_df = pd.read_csv(train_path)  
valid_df = pd.read_csv(valid_path)  

# train_df = train_df[:10]
# valid_df = valid_df[:1]
# len(valid_df)




class Dataset(Dataset):
    def __init__(self, df):
        self.df = df

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        text = self.df.iloc[idx, 0]
        #         print(text)

        label = self.df.iloc[idx, 1]
        #         print(label)
        return text, label


train_dataset = Dataset(train_df)  
valid_dataset = Dataset(valid_df)  #

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

                          
work_date = datetime.today().strftime("%Y%m%d_%H%M%S")
print(work_date)
model_save_dir = './saved_model/' + work_date  # ./saved_model/2021712)
print(model_save_dir)

optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
# criterion = torch.nn.CrossEntropyLoss()
model.to(device)  # model.cuda()
# model = Model(bert.roberta).to(device)


import os
def createFolder(directory):
    try:
        if not os.path.exists(directory):
            os.makedirs(directory)
    except OSError:
        print('Error: Creating directory. ' + directory)
createFolder(model_save_dir)




def train(epoch):
    model.train()
    total_loss = 0
    total_len = 0
    total_correct = 0
    itr = 1
    p_itr = 100
#     p_itr = 1

    for text, label in train_loader:
        optimizer.zero_grad()
        #     if len(text) > 512:
        #         print(text)
        encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]

        padded_list = [e + [0] * (512 - len(e)) for e in encoded_list]

        sample = torch.tensor(padded_list)

        sample, label = sample.to(device), label.to(device)

        labels = label.clone().detach()

        outputs = model(sample, labels=labels)

        loss, logits = outputs

        pred = torch.argmax(F.softmax(logits, dim=1), dim=1)

        correct = pred.eq(labels)

        total_correct += correct.sum().item()

        total_len += len(labels)  # 배치사이즈

        total_loss += loss.item()

        loss.backward()
        #     torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        #     scheduler.step()

        if itr % p_itr == 0:  # p_itr=2이면 1에폭(itr)안에서 배치사이즈만큼 2번돌때 출력
            print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.4f}'.format \
                      (epoch + 1, epochs, itr, total_loss / p_itr, total_correct / total_len))
            total_loss = 0
            total_len = 0
            total_correct = 0
        itr += 1

    model_epoch = 'epoch_{}.pt'.format(epoch + 1)
    #         print(model_epoch)
    final_save_dir = os.path.join(model_save_dir, model_epoch)
    #         print(final_save_dir)
    #         os.mkdir(final_save_dir)

    torch.save({
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict()
    }, final_save_dir)

    print("=> {}/{} saving checkpoint".format(epoch + 1, epochs))




def test():
    model.eval()

    total_loss = 0
    total_len = 0
    total_correct = 0

    for text, label in valid_loader:
        # print("Running Validation...")

        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]
        padded_list = [e + [0] * (512 - len(e)) for e in encoded_list]

        sample = torch.tensor(padded_list)
        sample, label = sample.to(device), label.to(device)
        labels = label.clone().detach()
        
        with torch.no_grad():

            outputs = model(sample, labels=labels)
        _, logits = outputs

        pred = torch.argmax(F.softmax(logits, dim=1), dim=1)
        correct = pred.eq(labels)
        total_correct += correct.sum().item()
        total_len += len(labels)

    print('Test accuracy: {:.4f}'.format(total_correct / total_len))




# main
start_epoch = 0
for epoch in range(start_epoch, epochs):
    train(epoch)

    test()
    now = time.gmtime(time.time() - start_time)
    print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))

train_date = datetime.today().strftime("%Y-%m-%d %H:%M:%S")
print(train_date)

now = time.gmtime(time.time() - start_time)
print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))

print("Training complete!")
"""
[Epoch 1/10] Iteration 500 -> Train Loss: 0.8380, Accuracy: 0.6826
=> 1/10 saving checkpoint
Test accuracy: 0.8681
0 hours 27 mins 39 secs for training
[Epoch 2/10] Iteration 500 -> Train Loss: 0.2519, Accuracy: 0.9144
=> 2/10 saving checkpoint
Test accuracy: 0.9164
0 hours 54 mins 52 secs for training
[Epoch 3/10] Iteration 500 -> Train Loss: 0.0925, Accuracy: 0.9712
=> 3/10 saving checkpoint
Test accuracy: 0.9275
1 hours 22 mins 15 secs for training
[Epoch 4/10] Iteration 500 -> Train Loss: 0.0457, Accuracy: 0.9869
=> 4/10 saving checkpoint
Test accuracy: 0.9362
1 hours 49 mins 28 secs for training
[Epoch 5/10] Iteration 500 -> Train Loss: 0.0379, Accuracy: 0.9902
=> 5/10 saving checkpoint
Test accuracy: 0.9357
2 hours 16 mins 40 secs for training
[Epoch 6/10] Iteration 500 -> Train Loss: 0.0320, Accuracy: 0.9891
=> 6/10 saving checkpoint
Test accuracy: 0.9364
2 hours 43 mins 56 secs for training
[Epoch 7/10] Iteration 500 -> Train Loss: 0.0302, Accuracy: 0.9914
=> 7/10 saving checkpoint
Test accuracy: 0.9308
3 hours 11 mins 24 secs for training
"""





