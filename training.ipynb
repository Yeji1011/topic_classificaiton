from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
# from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AdamW
# from transformers import BertTokenizer, RobertaForSequenceClassification
from pytorch_transformers import BertTokenizer, BertForSequenceClassification, BertConfig

# from torch.optim import Adam
import torch.nn.functional as F
import time
import os
import datetime
from datetime import datetime
from transformers import get_linear_schedule_with_warmup
import random  # 재현을 위해 랜덤시드 고정
import tqdm

os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # Uses GPU 0.
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
print('Available device : ', torch.cuda.device_count())
print('Current cuda device :', torch.cuda.current_device())
print(torch.cuda.get_device_name(device))


# Setting parameters
batch_size = 32
num_label = 7
learning_rate = 1e-5
max_grad_norm = 1
epochs = 10
num_workers = 8
max_len = 512



start_time = time.time()

# from transformers import BertTokenizer, RobertaTokenizer, RobertaForSequenceClassification
import torch

config = 'bert-base-multilingual-cased'
tokenizer = BertTokenizer.from_pretrained(config)
model = BertForSequenceClassification.from_pretrained(config, num_labels=num_label)


import pandas as pd

train_path ='./experiment_dataset/topic_train_dataset.csv'
valid_path = './experiment_dataset/topic_valid_dataset.csv'

train_df = pd.read_csv(train_path)  
valid_df = pd.read_csv(valid_path)  

# train_df = train_df[:10]
# valid_df = valid_df[:1]
# len(valid_df)

class Dataset(Dataset):
    def __init__(self, df):
        self.df = df

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        text = self.df.iloc[idx, 0]
        #         print(text)

        label = self.df.iloc[idx, 1]
        #         print(label)
        return text, label


train_dataset = Dataset(train_df)  
valid_dataset = Dataset(valid_df)  #

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

                          
work_date = datetime.today().strftime("%Y%m%d_%H%M%S")
print(work_date)
model_save_dir = './saved_model/' + work_date  # ./saved_model/2021712)
print(model_save_dir)

optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
# criterion = torch.nn.CrossEntropyLoss()
model.to(device)  # model.cuda()
# model = Model(bert.roberta).to(device)


import os
def createFolder(directory):
    try:
        if not os.path.exists(directory):
            os.makedirs(directory)
    except OSError:
        print('Error: Creating directory. ' + directory)
createFolder(model_save_dir)



def train(epoch):
    model.train()
    total_loss = 0
    total_len = 0
    total_correct = 0
    itr = 1
    p_itr = 100
#     p_itr = 1

    for text, label in train_loader:
        optimizer.zero_grad()
        #     if len(text) > 512:
        #         print(text)
#         encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]

#         padded_list = [e + [0] * (512 - len(e)) for e in encoded_list]


        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]
        #         print(encoded_list)
        padded_list = [e + [0] * (512 - len(e)) for e in encoded_list]
        
        sample = torch.tensor(padded_list)

        sample, label = sample.to(device), label.to(device)

        labels = label.clone().detach()

        outputs = model(sample, labels=labels)

        loss, logits = outputs

        pred = torch.argmax(F.softmax(logits, dim=1), dim=1)

        correct = pred.eq(labels)

        total_correct += correct.sum().item()

        total_len += len(labels)  # 배치사이즈

        total_loss += loss.item()

        loss.backward()
        #     torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        #     scheduler.step()

        if itr % p_itr == 0:  # p_itr=2이면 1에폭(itr)안에서 배치사이즈만큼 2번돌때 출력
            print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.4f}'.format \
                      (epoch + 1, epochs, itr, total_loss / p_itr, total_correct / total_len))
            total_loss = 0
            total_len = 0
            total_correct = 0
        itr += 1

    model_epoch = 'epoch_{}.pt'.format(epoch + 1)
    #         print(model_epoch)
    final_save_dir = os.path.join(model_save_dir, model_epoch)
    #         print(final_save_dir)
    #         os.mkdir(final_save_dir)

    torch.save({
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict()
    }, final_save_dir)

    print("=> {}/{} saving checkpoint".format(epoch + 1, epochs))
    
    
    
def test():
    model.eval()

    total_loss = 0
    total_len = 0
    total_correct = 0

    for text, label in valid_loader:
        # print("Running Validation...")

        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]
        padded_list = [e + [0] * (512 - len(e)) for e in encoded_list]

        sample = torch.tensor(padded_list)
        sample, label = sample.to(device), label.to(device)
        labels = label.clone().detach()
        
        with torch.no_grad():

            outputs = model(sample, labels=labels)
        _, logits = outputs

        pred = torch.argmax(F.softmax(logits, dim=1), dim=1)
        correct = pred.eq(labels)
        total_correct += correct.sum().item()
        total_len += len(labels)

    print('Test accuracy: {:.4f}'.format(total_correct / total_len))
    
    
# main
start_epoch = 0
for epoch in range(start_epoch, epochs):
    train(epoch)

    test()
    now = time.gmtime(time.time() - start_time)
    print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))

train_date = datetime.today().strftime("%Y-%m-%d %H:%M:%S")
print(train_date)

now = time.gmtime(time.time() - start_time)
print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))

print("Training complete!")

"""
[Epoch 1/10] Iteration 100 -> Train Loss: 1.5756, Accuracy: 0.3681
[Epoch 1/10] Iteration 200 -> Train Loss: 1.1680, Accuracy: 0.5347
[Epoch 1/10] Iteration 300 -> Train Loss: 0.9741, Accuracy: 0.6322
=> 1/10 saving checkpoint
Test accuracy: 0.7273
0 hours 9 mins 25 secs for training
[Epoch 2/10] Iteration 100 -> Train Loss: 0.7111, Accuracy: 0.7406
[Epoch 2/10] Iteration 200 -> Train Loss: 0.6786, Accuracy: 0.7491
[Epoch 2/10] Iteration 300 -> Train Loss: 0.6255, Accuracy: 0.7750
=> 2/10 saving checkpoint
Test accuracy: 0.8017
0 hours 18 mins 41 secs for training
[Epoch 3/10] Iteration 100 -> Train Loss: 0.5031, Accuracy: 0.8197
[Epoch 3/10] Iteration 200 -> Train Loss: 0.4642, Accuracy: 0.8356
[Epoch 3/10] Iteration 300 -> Train Loss: 0.4516, Accuracy: 0.8403
=> 3/10 saving checkpoint
Test accuracy: 0.8017
0 hours 27 mins 57 secs for training
[Epoch 4/10] Iteration 100 -> Train Loss: 0.3450, Accuracy: 0.8794
[Epoch 4/10] Iteration 200 -> Train Loss: 0.3330, Accuracy: 0.8847
[Epoch 4/10] Iteration 300 -> Train Loss: 0.3198, Accuracy: 0.8906
=> 4/10 saving checkpoint
Test accuracy: 0.8564
0 hours 37 mins 16 secs for training
[Epoch 5/10] Iteration 100 -> Train Loss: 0.2491, Accuracy: 0.9103
[Epoch 5/10] Iteration 200 -> Train Loss: 0.2416, Accuracy: 0.9166
[Epoch 5/10] Iteration 300 -> Train Loss: 0.2366, Accuracy: 0.9184
=> 5/10 saving checkpoint
Test accuracy: 0.8870
0 hours 46 mins 34 secs for training
[Epoch 6/10] Iteration 100 -> Train Loss: 0.1791, Accuracy: 0.9425
[Epoch 6/10] Iteration 200 -> Train Loss: 0.1655, Accuracy: 0.9456
[Epoch 6/10] Iteration 300 -> Train Loss: 0.1807, Accuracy: 0.9403
=> 6/10 saving checkpoint
Test accuracy: 0.9013
0 hours 55 mins 52 secs for training
[Epoch 7/10] Iteration 100 -> Train Loss: 0.1095, Accuracy: 0.9644
[Epoch 7/10] Iteration 200 -> Train Loss: 0.1192, Accuracy: 0.9609
[Epoch 7/10] Iteration 300 -> Train Loss: 0.1319, Accuracy: 0.9513
=> 7/10 saving checkpoint
Test accuracy: 0.9013
1 hours 5 mins 10 secs for training
[Epoch 8/10] Iteration 100 -> Train Loss: 0.1088, Accuracy: 0.9647
[Epoch 8/10] Iteration 200 -> Train Loss: 0.0869, Accuracy: 0.9697
[Epoch 8/10] Iteration 300 -> Train Loss: 0.0935, Accuracy: 0.9719
=> 8/10 saving checkpoint
Test accuracy: 0.9042
1 hours 14 mins 28 secs for training
[Epoch 9/10] Iteration 100 -> Train Loss: 0.0625, Accuracy: 0.9809
[Epoch 9/10] Iteration 200 -> Train Loss: 0.0785, Accuracy: 0.9769
[Epoch 9/10] Iteration 300 -> Train Loss: 0.0708, Accuracy: 0.9778
=> 9/10 saving checkpoint
Test accuracy: 0.8957
1 hours 28 mins 15 secs for training
[Epoch 10/10] Iteration 100 -> Train Loss: 0.1193, Accuracy: 0.9650
[Epoch 10/10] Iteration 200 -> Train Loss: 0.0564, Accuracy: 0.9819
[Epoch 10/10] Iteration 300 -> Train Loss: 0.0599, Accuracy: 0.9819
=> 10/10 saving checkpoint
Test accuracy: 0.9126
1 hours 47 mins 28 secs for training
2021-11-28 01:40:29
1 hours 47 mins 28 secs for training
Training complete!
"""
