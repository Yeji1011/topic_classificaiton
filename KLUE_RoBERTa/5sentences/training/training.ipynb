=> 6/10 saving checkpoint
Test accuracy: 0.9800


from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
# from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AdamW
from transformers import BertTokenizer, RobertaForSequenceClassification

# from torch.optim import Adam
import torch.nn.functional as F
import time
import os
import datetime
from datetime import datetime
from transformers import get_linear_schedule_with_warmup
import random  # 재현을 위해 랜덤시드 고정
import tqdm

os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # Uses GPU 0.
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
print('Available device : ', torch.cuda.device_count())
print('Current cuda device :', torch.cuda.current_device())
print(torch.cuda.get_device_name(device))


# Setting parameters
batch_size = 16
num_label = 7
learning_rate = 1e-5
max_grad_norm = 1
epochs = 10
num_workers = 8
# max_len = 512


start_time = time.time()

from transformers import BertTokenizer, RobertaTokenizer, RobertaForSequenceClassification
import torch

config = 'klue/roberta-large'
tokenizer = BertTokenizer.from_pretrained(config)
model = RobertaForSequenceClassification.from_pretrained(config, num_labels=num_label)


import pandas as pd

train_path ='./experiment_dataset/topic_train_dataset.csv'
valid_path = './experiment_dataset/topic_valid_dataset.csv'

train_df = pd.read_csv(train_path)  
valid_df = pd.read_csv(valid_path)  

# train_df = train_df[:10]
# valid_df = valid_df[:1]
# len(valid_df)

class Dataset(Dataset):
    def __init__(self, df):
        self.df = df

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        text = self.df.iloc[idx, 0]
        #         print(text)

        label = self.df.iloc[idx, 1]
        #         print(label)
        return text, label


train_dataset = Dataset(train_df)  
valid_dataset = Dataset(valid_df)  #

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

                          
work_date = datetime.today().strftime("%Y%m%d_%H%M%S")
print(work_date)
model_save_dir = './saved_model/' + work_date  # ./saved_model/2021712)
print(model_save_dir)

optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
# criterion = torch.nn.CrossEntropyLoss()
model.to(device)  # model.cuda()
# model = Model(bert.roberta).to(device)


import os
def createFolder(directory):
    try:
        if not os.path.exists(directory):
            os.makedirs(directory)
    except OSError:
        print('Error: Creating directory. ' + directory)
createFolder(model_save_dir)



def train(epoch):
    model.train()
    total_loss = 0
    total_len = 0
    total_correct = 0
    itr = 1
    p_itr = 100
#     p_itr = 1

    for text, label in train_loader:
        optimizer.zero_grad()
        #     if len(text) > 512:
        #         print(text)
        encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]

        padded_list = [e + [0] * (512 - len(e)) for e in encoded_list]

        sample = torch.tensor(padded_list)

        sample, label = sample.to(device), label.to(device)

        labels = label.clone().detach()

        outputs = model(sample, labels=labels)

        loss, logits = outputs

        pred = torch.argmax(F.softmax(logits, dim=1), dim=1)

        correct = pred.eq(labels)

        total_correct += correct.sum().item()

        total_len += len(labels)  # 배치사이즈

        total_loss += loss.item()

        loss.backward()
        #     torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        #     scheduler.step()

        if itr % p_itr == 0:  # p_itr=2이면 1에폭(itr)안에서 배치사이즈만큼 2번돌때 출력
            print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.4f}'.format \
                      (epoch + 1, epochs, itr, total_loss / p_itr, total_correct / total_len))
            total_loss = 0
            total_len = 0
            total_correct = 0
        itr += 1

    model_epoch = 'epoch_{}.pt'.format(epoch + 1)
    #         print(model_epoch)
    final_save_dir = os.path.join(model_save_dir, model_epoch)
    #         print(final_save_dir)
    #         os.mkdir(final_save_dir)

    torch.save({
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict()
    }, final_save_dir)

    print("=> {}/{} saving checkpoint".format(epoch + 1, epochs))
    
    
def test():
    model.eval()

    total_loss = 0
    total_len = 0
    total_correct = 0

    for text, label in valid_loader:
        # print("Running Validation...")

        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]
        padded_list = [e + [0] * (512 - len(e)) for e in encoded_list]

        sample = torch.tensor(padded_list)
        sample, label = sample.to(device), label.to(device)
        labels = label.clone().detach()
        
        with torch.no_grad():

            outputs = model(sample, labels=labels)
        _, logits = outputs

        pred = torch.argmax(F.softmax(logits, dim=1), dim=1)
        correct = pred.eq(labels)
        total_correct += correct.sum().item()
        total_len += len(labels)

    print('Test accuracy: {:.4f}'.format(total_correct / total_len))
    



    
# main
start_epoch = 0
for epoch in range(start_epoch, epochs):
    train(epoch)

    test()
    now = time.gmtime(time.time() - start_time)
    print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))

train_date = datetime.today().strftime("%Y-%m-%d %H:%M:%S")
print(train_date)

now = time.gmtime(time.time() - start_time)
print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))

print("Training complete!")

"""
[Epoch 1/10] Iteration 100 -> Train Loss: 1.4063, Accuracy: 0.4381
[Epoch 1/10] Iteration 200 -> Train Loss: 0.7011, Accuracy: 0.7625
[Epoch 1/10] Iteration 300 -> Train Loss: 0.4868, Accuracy: 0.8363
[Epoch 1/10] Iteration 400 -> Train Loss: 0.3850, Accuracy: 0.8694
[Epoch 1/10] Iteration 500 -> Train Loss: 0.3409, Accuracy: 0.8881
[Epoch 1/10] Iteration 600 -> Train Loss: 0.3024, Accuracy: 0.8912
[Epoch 1/10] Iteration 700 -> Train Loss: 0.2492, Accuracy: 0.9175
=> 1/10 saving checkpoint
Test accuracy: 0.9347
0 hours 27 mins 22 secs for training
[Epoch 2/10] Iteration 100 -> Train Loss: 0.1463, Accuracy: 0.9537
[Epoch 2/10] Iteration 200 -> Train Loss: 0.1363, Accuracy: 0.9550
[Epoch 2/10] Iteration 300 -> Train Loss: 0.1027, Accuracy: 0.9663
[Epoch 2/10] Iteration 400 -> Train Loss: 0.1096, Accuracy: 0.9644
[Epoch 2/10] Iteration 500 -> Train Loss: 0.1003, Accuracy: 0.9731
[Epoch 2/10] Iteration 600 -> Train Loss: 0.1197, Accuracy: 0.9619
[Epoch 2/10] Iteration 700 -> Train Loss: 0.1192, Accuracy: 0.9613
=> 2/10 saving checkpoint
Test accuracy: 0.9546
0 hours 54 mins 20 secs for training
[Epoch 3/10] Iteration 100 -> Train Loss: 0.0672, Accuracy: 0.9800
[Epoch 3/10] Iteration 200 -> Train Loss: 0.0447, Accuracy: 0.9875
[Epoch 3/10] Iteration 300 -> Train Loss: 0.0463, Accuracy: 0.9856
[Epoch 3/10] Iteration 400 -> Train Loss: 0.0421, Accuracy: 0.9875
[Epoch 3/10] Iteration 500 -> Train Loss: 0.0577, Accuracy: 0.9800
[Epoch 3/10] Iteration 600 -> Train Loss: 0.0513, Accuracy: 0.9838
[Epoch 3/10] Iteration 700 -> Train Loss: 0.0534, Accuracy: 0.9825
=> 3/10 saving checkpoint
Test accuracy: 0.9760
1 hours 21 mins 26 secs for training
[Epoch 4/10] Iteration 100 -> Train Loss: 0.0149, Accuracy: 0.9950
[Epoch 4/10] Iteration 200 -> Train Loss: 0.0231, Accuracy: 0.9962
[Epoch 4/10] Iteration 300 -> Train Loss: 0.0239, Accuracy: 0.9919
[Epoch 4/10] Iteration 400 -> Train Loss: 0.0362, Accuracy: 0.9875
[Epoch 4/10] Iteration 500 -> Train Loss: 0.0339, Accuracy: 0.9900
[Epoch 4/10] Iteration 600 -> Train Loss: 0.0278, Accuracy: 0.9906
[Epoch 4/10] Iteration 700 -> Train Loss: 0.0361, Accuracy: 0.9894
=> 4/10 saving checkpoint
Test accuracy: 0.9617
1 hours 48 mins 33 secs for training
[Epoch 5/10] Iteration 100 -> Train Loss: 0.0148, Accuracy: 0.9962
[Epoch 5/10] Iteration 200 -> Train Loss: 0.0233, Accuracy: 0.9938
[Epoch 5/10] Iteration 300 -> Train Loss: 0.0395, Accuracy: 0.9881
[Epoch 5/10] Iteration 400 -> Train Loss: 0.0241, Accuracy: 0.9912
[Epoch 5/10] Iteration 500 -> Train Loss: 0.0272, Accuracy: 0.9906
[Epoch 5/10] Iteration 600 -> Train Loss: 0.0203, Accuracy: 0.9944
[Epoch 5/10] Iteration 700 -> Train Loss: 0.0180, Accuracy: 0.9956
=> 5/10 saving checkpoint
Test accuracy: 0.9772
2 hours 15 mins 28 secs for training
[Epoch 6/10] Iteration 100 -> Train Loss: 0.0263, Accuracy: 0.9894
[Epoch 6/10] Iteration 200 -> Train Loss: 0.0091, Accuracy: 0.9981
[Epoch 6/10] Iteration 300 -> Train Loss: 0.0049, Accuracy: 0.9988
[Epoch 6/10] Iteration 400 -> Train Loss: 0.0082, Accuracy: 0.9988
[Epoch 6/10] Iteration 500 -> Train Loss: 0.0023, Accuracy: 0.9994
[Epoch 6/10] Iteration 600 -> Train Loss: 0.0120, Accuracy: 0.9975
[Epoch 6/10] Iteration 700 -> Train Loss: 0.0111, Accuracy: 0.9969
=> 6/10 saving checkpoint
Test accuracy: 0.9800
2 hours 42 mins 32 secs for training
[Epoch 7/10] Iteration 100 -> Train Loss: 0.0133, Accuracy: 0.9944
[Epoch 7/10] Iteration 200 -> Train Loss: 0.0148, Accuracy: 0.9950
[Epoch 7/10] Iteration 300 -> Train Loss: 0.0219, Accuracy: 0.9944
[Epoch 7/10] Iteration 400 -> Train Loss: 0.0247, Accuracy: 0.9912
[Epoch 7/10] Iteration 500 -> Train Loss: 0.0180, Accuracy: 0.9938
[Epoch 7/10] Iteration 600 -> Train Loss: 0.0350, Accuracy: 0.9875
[Epoch 7/10] Iteration 700 -> Train Loss: 0.0366, Accuracy: 0.9875
=> 7/10 saving checkpoint
Test accuracy: 0.9739
3 hours 9 mins 24 secs for training
[Epoch 8/10] Iteration 100 -> Train Loss: 0.0089, Accuracy: 0.9981
[Epoch 8/10] Iteration 200 -> Train Loss: 0.0099, Accuracy: 0.9969
[Epoch 8/10] Iteration 300 -> Train Loss: 0.0275, Accuracy: 0.9944
[Epoch 8/10] Iteration 400 -> Train Loss: 0.0060, Accuracy: 0.9981
[Epoch 8/10] Iteration 500 -> Train Loss: 0.0066, Accuracy: 0.9981
[Epoch 8/10] Iteration 600 -> Train Loss: 0.0071, Accuracy: 0.9975
[Epoch 8/10] Iteration 700 -> Train Loss: 0.0121, Accuracy: 0.9962
=> 8/10 saving checkpoint
Test accuracy: 0.9743
3 hours 36 mins 32 secs for training
[Epoch 9/10] Iteration 100 -> Train Loss: 0.0335, Accuracy: 0.9900
[Epoch 9/10] Iteration 200 -> Train Loss: 0.0214, Accuracy: 0.9925
[Epoch 9/10] Iteration 300 -> Train Loss: 0.0296, Accuracy: 0.9931
[Epoch 9/10] Iteration 400 -> Train Loss: 0.0205, Accuracy: 0.9956
[Epoch 9/10] Iteration 500 -> Train Loss: 0.0118, Accuracy: 0.9956
[Epoch 9/10] Iteration 600 -> Train Loss: 0.0312, Accuracy: 0.9894
[Epoch 9/10] Iteration 700 -> Train Loss: 0.0190, Accuracy: 0.9919
=> 9/10 saving checkpoint
Test accuracy: 0.9798
4 hours 3 mins 38 secs for training
[Epoch 10/10] Iteration 100 -> Train Loss: 0.0021, Accuracy: 1.0000
[Epoch 10/10] Iteration 200 -> Train Loss: 0.0026, Accuracy: 0.9988
[Epoch 10/10] Iteration 300 -> Train Loss: 0.0061, Accuracy: 0.9981
[Epoch 10/10] Iteration 400 -> Train Loss: 0.0109, Accuracy: 0.9962
[Epoch 10/10] Iteration 500 -> Train Loss: 0.0321, Accuracy: 0.9906
[Epoch 10/10] Iteration 600 -> Train Loss: 0.0183, Accuracy: 0.9975
[Epoch 10/10] Iteration 700 -> Train Loss: 0.0234, Accuracy: 0.9944
=> 10/10 saving checkpoint
Test accuracy: 0.9748
4 hours 30 mins 41 secs for training
2021-11-28 16:14:05
4 hours 30 mins 41 secs for training
Training complete!
"""
